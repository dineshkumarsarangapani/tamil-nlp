{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc.ta.300.bin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.util.download_model('ta', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.ta.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.07138658e-02, -1.08233422e-01,  8.64246711e-02, -3.88603681e-03,\n",
       "       -2.33822837e-02, -5.32085709e-02,  7.82545730e-02,  1.84746664e-02,\n",
       "       -1.94862578e-02,  2.73230970e-02, -2.71674665e-03, -2.83441171e-02,\n",
       "       -1.59673598e-02, -8.13780166e-03,  4.37758118e-02, -1.29452512e-01,\n",
       "       -4.93319593e-02, -7.74425268e-02,  8.85141492e-02,  4.18095253e-02,\n",
       "        2.46369764e-02, -3.11867390e-02,  3.22013982e-02, -9.67689231e-02,\n",
       "       -7.05196261e-02, -9.63690318e-03,  4.09294665e-02,  1.01436786e-02,\n",
       "        7.92189501e-04,  3.93997692e-02,  3.72402966e-02,  2.02535633e-02,\n",
       "        6.97395578e-02,  2.53558289e-02, -3.79315093e-02,  2.80181412e-02,\n",
       "       -1.37251854e-01,  1.04167117e-02,  3.35209444e-02,  8.54814425e-03,\n",
       "       -1.01716286e-02, -1.81007888e-02, -1.82577930e-02,  5.56269214e-02,\n",
       "       -3.24874595e-02,  5.49678728e-02,  1.01667553e-01, -2.81709246e-03,\n",
       "        4.82380204e-02, -9.16458853e-03,  3.25434729e-02, -1.28710642e-03,\n",
       "        1.43677555e-02, -5.01727946e-02,  6.23157853e-03,  8.87008980e-02,\n",
       "        5.61366752e-02, -1.01055562e-01,  6.22094423e-03, -8.00869986e-03,\n",
       "        1.21272728e-03, -5.44417650e-02, -2.51762196e-02, -2.90403068e-02,\n",
       "        2.27600373e-02,  4.07213271e-02, -3.82254571e-02, -3.91630009e-02,\n",
       "        2.62755640e-02,  3.92581522e-02,  6.86519593e-02, -2.23755278e-02,\n",
       "        6.31741434e-03, -3.07353102e-02, -1.06733702e-02, -6.10712171e-03,\n",
       "       -5.75775839e-02,  3.69629078e-02,  3.97820733e-02, -1.71363503e-02,\n",
       "        2.61285272e-03,  8.29600617e-02, -4.32946123e-02, -2.16537677e-02,\n",
       "        5.05190250e-03,  4.31601070e-02, -6.05503842e-02, -4.54037562e-02,\n",
       "        4.88588959e-02, -2.45557241e-02, -2.99588945e-02, -2.44877189e-02,\n",
       "       -4.89428304e-02,  1.10519961e-01,  1.61115304e-02,  2.61662491e-02,\n",
       "       -4.85871220e-03, -1.39171146e-02, -4.03932594e-02, -3.58050317e-02,\n",
       "        2.33235769e-03, -3.83919366e-02, -1.36843175e-02,  5.55072054e-02,\n",
       "       -6.95222989e-04,  3.15144360e-02,  3.54591385e-02,  4.93224226e-02,\n",
       "       -7.08303377e-02, -1.26210228e-02,  2.47039124e-02, -1.43407919e-02,\n",
       "        8.07057172e-02, -5.80528677e-02, -5.17278016e-02,  2.30621416e-02,\n",
       "       -2.59380359e-02,  3.30888443e-02,  8.11296329e-03,  6.34716973e-02,\n",
       "       -5.04235961e-02,  1.15722697e-02,  3.60996947e-02,  4.99828439e-03,\n",
       "        4.22487035e-03,  6.35060146e-02, -6.98826239e-02, -3.75882909e-02,\n",
       "       -2.19102949e-05, -3.92355770e-02,  1.22506116e-02,  4.46098372e-02,\n",
       "        2.14569513e-02,  9.26581621e-02,  1.07229888e-01, -3.25065255e-02,\n",
       "        1.12450421e-02, -6.81511089e-02, -7.36031979e-02, -3.28803100e-02,\n",
       "       -3.87134925e-02, -3.38665098e-02,  1.23784607e-02, -1.06986761e-01,\n",
       "       -1.10271312e-02, -2.66744699e-02, -8.04119036e-02, -4.37113903e-02,\n",
       "        4.63658795e-02, -4.40542474e-02,  6.50198013e-02,  1.04504988e-01,\n",
       "       -6.19208515e-02, -2.29271650e-02,  4.21993472e-02, -2.59068459e-02,\n",
       "        8.47659819e-03, -1.03412569e-02, -9.31768417e-02, -6.73630759e-02,\n",
       "        2.25409195e-02, -8.46631266e-03, -1.47188410e-01, -1.00061512e-02,\n",
       "       -6.03266656e-02, -6.41748607e-02, -1.16655361e-02,  9.75296199e-02,\n",
       "       -2.77636647e-02, -6.71337023e-02, -9.17537212e-02,  3.09326127e-03,\n",
       "        4.18329909e-02,  1.80750936e-01,  2.31816918e-02, -9.11522955e-02,\n",
       "        1.86241549e-02, -1.16894677e-01, -1.77119933e-02, -6.84000403e-02,\n",
       "       -1.51350051e-01, -6.79131001e-02,  8.90331939e-02,  4.56832163e-02,\n",
       "       -7.64571317e-03, -9.27434042e-02,  9.61043835e-02, -1.01552363e-02,\n",
       "        4.40083444e-02, -6.47618175e-02, -1.65424012e-02, -1.21809263e-02,\n",
       "       -6.61015138e-02,  9.58269015e-02, -8.27319548e-02,  1.57611705e-02,\n",
       "        5.73168322e-03, -3.61405127e-02, -5.51757663e-02,  6.35012053e-03,\n",
       "        2.03932524e-02,  1.30645316e-02, -8.35498795e-03,  2.09228341e-02,\n",
       "        4.95659448e-02,  6.37451559e-02,  1.33966189e-02,  5.76845072e-02,\n",
       "       -1.60518885e-02,  9.74556431e-03,  7.68591976e-03, -3.21670547e-02,\n",
       "        6.10857159e-02,  3.46268341e-02, -2.36341991e-02,  4.88484390e-02,\n",
       "        1.09535187e-01,  1.87067017e-02,  2.84333266e-02, -5.00950031e-02,\n",
       "        1.03752583e-01, -4.69273143e-03,  5.04147820e-03, -3.62856910e-02,\n",
       "        1.12492740e-01, -4.74103838e-02, -9.40134823e-02,  6.95302337e-02,\n",
       "        5.10554425e-02, -2.54581645e-02, -3.01618385e-03,  7.69941062e-02,\n",
       "       -2.63961684e-03, -1.24371331e-03, -1.02695301e-01,  5.55322617e-02,\n",
       "        6.57050591e-03,  2.92668305e-03,  6.12319894e-02,  3.42014767e-02,\n",
       "       -2.45647691e-02,  1.17106158e-02, -4.54090089e-02,  3.29092517e-02,\n",
       "       -4.04625610e-02,  1.08906729e-02, -6.48228452e-02, -2.76252702e-02,\n",
       "       -7.10855126e-02,  3.00445184e-02, -1.55694783e-03,  3.14612240e-02,\n",
       "        1.48122534e-01,  6.75457530e-04,  8.31954647e-03,  4.66772877e-02,\n",
       "       -6.39874488e-02,  5.98157570e-02,  1.04828421e-02,  4.15737741e-04,\n",
       "        5.64593673e-02, -3.10326852e-02,  6.62625283e-02, -1.26074910e-01,\n",
       "        2.72270069e-02, -5.06682172e-02,  1.41158458e-02,  1.92318503e-02,\n",
       "        3.15858237e-03, -2.67344229e-02,  4.68411250e-03,  3.54605988e-02,\n",
       "       -1.44775063e-02, -2.96220835e-03, -7.13729439e-03,  7.06955045e-03,\n",
       "        8.26593935e-02, -2.91176736e-02, -8.39022249e-02,  5.86431697e-02,\n",
       "       -1.12313345e-01,  2.66017653e-02,  3.13926078e-02, -1.03432685e-04,\n",
       "        1.67514347e-02, -2.36176811e-02,  1.42269898e-02,  6.18775934e-02,\n",
       "        2.99464315e-02, -2.18138453e-02,  5.53271025e-02,  1.42988167e-03,\n",
       "       -6.99964538e-02,  4.05655690e-02,  1.53280497e-02,  4.37130444e-02,\n",
       "        2.91982442e-02, -2.44746599e-02,  1.09086856e-02, -6.09413609e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector('பள்ளி')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "அக்னி குஞ்சொன்று கண்டேன்....\n",
      "அக்னி குஞ்சொன்று கண்டேன் - அதை \n",
      "அங்கொரு காட்டிலோர் பொந்திடை வைத்தேன்\n",
      "\n",
      "அக்னி குஞ்சொன்று கண்டேன் - அதை \n",
      "அங்கொரு காட்டிலோர் பொந்திடை வைத்தேன்\n",
      "\n",
      "வெந்து தணிந்தது காடு \n",
      "வெந்து தண\n",
      "['அக்னி', 'குஞ்சொன்று', 'கண்டேன்', 'அக்னி', 'குஞ்சொன்று', 'கண்டேன்', '', 'அதை', 'அங்கொரு', 'காட்டிலோர்', 'பொந்திடை', 'வைத்தேன்', 'அக்னி', 'குஞ்சொன்று', 'கண்டேன்', '', 'அதை', 'அங்கொரு', 'காட்டிலோர்', 'பொந்திடை', 'வைத்தேன்', 'வெந்து', 'தணிந்தது', 'காடு', 'வெந்து', 'தணிந்தது', 'காடு', '', 'தழல்', 'வீரதிற்குஞ்சென்று', 'மூபென்றுமுண்டோ', 'தத்தரிகிட', 'தத்தரிகிட', 'தித்தோம்', 'தக', 'தத்தரிகிட', 'தத்தரிகிட', 'தித்தோம்', 'அக்னி', 'குஞ்சொன்று', 'கண்டேன்', '', 'அதை', 'அங்கொரு', 'காட்டிலோர்', 'பொந்திடை', 'வைத்தேன்', 'வெட்டி', 'அடிக்குது', 'மின்னல்']\n",
      "Total Tokens: 2015\n",
      "Unique Tokens: 830\n",
      "Total Sequences: 1989\n",
      "Sample Sequences அக்னி குஞ்சொன்று கண்டேன் அக்னி குஞ்சொன்று கண்டேன்  அதை அங்கொரு காட்டிலோர் பொந்திடை வைத்தேன் அக்னி குஞ்சொன்று கண்டேன்  அதை அங்கொரு காட்டிலோர் பொந்திடை வைத்தேன் வெந்து தணிந்தது காடு வெந்து தணிந்தது\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\t#tokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "    # Strip all words\n",
    "    #tokens = [word.strip() for word in tokens]\n",
    "\treturn tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'bharathi.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:50])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 25 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print(\"Sample Sequences\", sequences[0])\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'bharathi_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'bharathi_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189 188 135 ... 404   0   0]\n",
      " [188 135 189 ... 403   0   0]\n",
      " [135 189 188 ...   0   0   0]\n",
      " ...\n",
      " [ 63  64  26 ... 829  21  22]\n",
      " [ 64  26  65 ...  21  22 829]\n",
      " [ 26  65  66 ...  22 829  21]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# define sequences\n",
    "# pad sequence\n",
    "sequences = pad_sequences(sequences, padding='post')\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 26)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 25, 50)            41500     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 25, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 830)               83830     \n",
      "=================================================================\n",
      "Total params: 276,230\n",
      "Trainable params: 276,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 6.5898 - accuracy: 0.2956\n",
      "Epoch 2/25\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 5.2279 - accuracy: 0.3243\n",
      "Epoch 3/25\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 4.8325 - accuracy: 0.3243\n",
      "Epoch 4/25\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 4.7558 - accuracy: 0.3248\n",
      "Epoch 5/25\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 4.6915 - accuracy: 0.3238\n",
      "Epoch 6/25\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 4.5574 - accuracy: 0.3203\n",
      "Epoch 7/25\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.4574 - accuracy: 0.3214Epoch 8/25\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 4.3823 - accuracy: 0.3233\n",
      "Epoch 9/25\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 4.3438 - accuracy: 0.3223\n",
      "Epoch 10/25\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 4.2880 - accuracy: 0.3248\n",
      "Epoch 11/25\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 4.1162 - accuracy: 0.3298\n",
      "Epoch 12/25\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 4.0170 - accuracy: 0.3278\n",
      "Epoch 13/25\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 3.9336 - accuracy: 0.3298\n",
      "Epoch 14/25\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 3.8785 - accuracy: 0.3308\n",
      "Epoch 15/25\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 3.8290 - accuracy: 0.3308\n",
      "Epoch 16/25\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 3.7934 - accuracy: 0.3303\n",
      "Epoch 17/25\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 3.7573 - accuracy: 0.3313\n",
      "Epoch 18/25\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 3.7375 - accuracy: 0.3313\n",
      "Epoch 19/25\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 3.6741 - accuracy: 0.3369\n",
      "Epoch 20/25\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 3.6144 - accuracy: 0.3369\n",
      "Epoch 21/25\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 3.5703 - accuracy: 0.3399\n",
      "Epoch 22/25\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 3.5179 - accuracy: 0.3414\n",
      "Epoch 23/25\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 3.4853 - accuracy: 0.3414\n",
      "Epoch 24/25\n",
      "16/16 [==============================] - 7203s 450s/step - loss: 3.4586 - accuracy: 0.3429\n",
      "Epoch 25/25\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 3.4281 - accuracy: 0.3439\n"
     ]
    }
   ],
   "source": [
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=25)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "வெள்ளி பனிமலையின் மீதுலாவுவோம் அடி மேலை கடல் முழுதும் கப்பல் விடுவோம் பள்ளி தளம் அனைத்தும் கோவில் செய்குவோம் பள்ளி தளம் அனைத்தும் கோவில் செய்குவோம் எங்கள் பாரத தேசம் என்று தோள் கொட்டுவோம் எங்கள்\n",
      "\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 25) for input Tensor(\"embedding_2_input_4:0\", shape=(None, 25), dtype=float32), but it was called on an input with incompatible shape (None, 23).\n",
      "                                                 \n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'bharathi_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 \n"
     ]
    }
   ],
   "source": [
    "seed_text = 'என்று'\n",
    "#seq_length = 10\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
